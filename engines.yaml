# engines.yaml is a YAML file that contains the configuration for the AI engines and models.
# Note: The default max tokens is used when the max tokens is not specified in the request.
# Updated: October 2025 with latest models
# max_input_tokens: Maximum tokens for input (context window)
# max_output_tokens: Maximum tokens the model can generate in response
# default_max_tokens: Recommended default with safety margin (typically 80% of max_output_tokens)
engines:
  - name: openai
    api_key_name: OPENAI_API_KEY
    models:
      # o3 Series - Most advanced reasoning models (2025)
      - name: o3-mini
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 200000
        max_output_tokens: 100000
        default_max_tokens: 8192
      - name: o3-pro
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 200000
        max_output_tokens: 100000
        default_max_tokens: 8192
      - name: o3-deep-research
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 200000
        max_output_tokens: 100000
        default_max_tokens: 8192
      # o1 Series - Advanced reasoning models (2024)
      - name: o1
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 200000
        max_output_tokens: 100000
        default_max_tokens: 8192
      - name: o1-pro
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 200000
        max_output_tokens: 100000
        default_max_tokens: 8192
      - name: o1-mini
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 128000
        max_output_tokens: 65536
        default_max_tokens: 8192
      - name: o1-preview
        supports_system_role: true
        max_temperature: 1
        temperature: 1
        max_input_tokens: 128000
        max_output_tokens: 32768
        default_max_tokens: 8192
      # GPT-4o - Latest multimodal flagship model
      - name: gpt-4o
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 16384
        default_max_tokens: 4096
      - name: gpt-4o-2024-11-20
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 16384
        default_max_tokens: 4096
      - name: gpt-4o-2024-08-06
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 16384
        default_max_tokens: 4096
      # GPT-4o Mini - Fast and cost-effective
      - name: gpt-4o-mini
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 16384
        default_max_tokens: 4096
      # GPT-4o Audio - Multimodal with audio support
      - name: gpt-4o-audio-preview
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 16384
        default_max_tokens: 4096
      - name: gpt-4o-mini-audio-preview
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 16384
        default_max_tokens: 4096
      # GPT-4 Turbo - Previous generation
      - name: gpt-4-turbo
        supports_system_role: true
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 128000
        max_output_tokens: 4096
        default_max_tokens: 4096
    default_model: gpt-4o
  - name: anthropic
    api_key_name: ANTHROPIC_API_KEY
    models:
      # Claude 4.5 Sonnet - Latest and most capable (Sep 2025)
      - name: claude-sonnet-4-5-20250929
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 64000
        default_max_tokens: 8192
      - name: claude-sonnet-4-5
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 64000
        default_max_tokens: 8192
      # Claude 4 Opus 4.1 - Most powerful reasoning (Aug 2025)
      - name: claude-opus-4-1-20250805
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 32000
        default_max_tokens: 8192
      - name: claude-opus-4-1
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 32000
        default_max_tokens: 8192
      # Claude 4 Sonnet - Extended context version (May 2025)
      - name: claude-sonnet-4-20250514
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 1000000
        max_output_tokens: 64000
        default_max_tokens: 8192
      - name: claude-4-sonnet-20250514
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 1000000
        max_output_tokens: 64000
        default_max_tokens: 8192
      # Claude 4 Opus - Complex tasks (May 2025)
      - name: claude-opus-4-20250514
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 32000
        default_max_tokens: 8192
      - name: claude-4-opus-20250514
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 32000
        default_max_tokens: 8192
      # Claude 3.7 Sonnet - Hybrid reasoning (Feb 2025)
      - name: claude-3-7-sonnet-20250219
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 128000
        default_max_tokens: 8192
      - name: claude-3-7-sonnet-latest
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 128000
        default_max_tokens: 8192
      # Claude 3.5 Sonnet - High performance (Oct 2024)
      - name: claude-3-5-sonnet-20241022
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 8192
        default_max_tokens: 8192
      - name: claude-3-5-sonnet-latest
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 8192
        default_max_tokens: 8192
      # Claude 3.5 Haiku - Fast and efficient
      - name: claude-3-5-haiku-20241022
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 8192
        default_max_tokens: 8192
      - name: claude-3-5-haiku-latest
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 8192
        default_max_tokens: 8192
      # Claude 3 Opus - Previous flagship
      - name: claude-3-opus-20240229
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 4096
        default_max_tokens: 4096
      - name: claude-3-opus-latest
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 4096
        default_max_tokens: 4096
      # Claude 3 Haiku - Fastest and most compact
      - name: claude-3-haiku-20240307
        supports_system_role: true
        max_temperature: 1
        temperature: 0.75
        max_input_tokens: 200000
        max_output_tokens: 4096
        default_max_tokens: 4096
    default_model: claude-sonnet-4-5-20250929
  - name: google
    api_key_name: GOOGLE_API_KEY
    models:
      # Gemini 2.5 Pro - Most capable reasoning model
      - name: gemini/gemini-2.5-pro
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      - name: gemini/gemini-pro-latest
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      # Gemini 2.5 Flash - Balanced speed and capability
      - name: gemini/gemini-2.5-flash
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      - name: gemini/gemini-flash-latest
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      # Gemini 2.5 Flash Lite - Ultra-fast responses
      - name: gemini/gemini-2.5-flash-lite
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      - name: gemini/gemini-flash-lite-latest
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      # Gemini 2.0 Flash - Previous generation flagship
      - name: gemini/gemini-2.0-flash
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 8192
        default_max_tokens: 8192
      - name: gemini/gemini-2.0-flash-001
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 8192
        default_max_tokens: 8192
      # Gemini 2.0 Flash Lite - Fast and lightweight
      - name: gemini/gemini-2.0-flash-lite
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 8192
        default_max_tokens: 8192
      # Gemini 2.0 Pro Experimental - Advanced capabilities
      - name: gemini/gemini-2.0-pro-exp
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      # Gemini 2.0 Flash Thinking - Reasoning-focused
      - name: gemini/gemini-2.0-flash-thinking-exp
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
      # Gemini Experimental Models
      - name: gemini/gemini-exp-1206
        supports_system_role: false
        max_temperature: 2
        temperature: 0.75
        max_input_tokens: 1048576
        max_output_tokens: 65536
        default_max_tokens: 8192
    default_model: gemini/gemini-2.5-flash

default: anthropic
# These are the preset creativity temperature settings for the engine and model.
temperature_settings:
  precise: 0.25
  balanced: 0.50
  creative: 0.75
